{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from os.path import dirname\n",
    "\n",
    "root_path = dirname(os.getcwd())\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "data_dir = root_path + \"/data/datasets/original/\"\n",
    "data_dir_processed = root_path + \"/data/datasets/processed/\"\n",
    "data_dir_graphs = root_path + \"/data/datasets/graphs_repair/\"\n",
    "\n",
    "print(root_path, data_dir, data_dir_processed, data_dir_graphs, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset_features.json\", 'r') as file:\n",
    "    datasets_info = json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(datasets_info.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"BPI12_DECLINED_COMPLETE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_all = pd.read_csv(f\"datasets/processed/{dataset}_processed_all.csv\")\n",
    "tab_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_train = pd.read_csv(f\"datasets/processed/{dataset}_processed_train.csv\")\n",
    "tab_valid = pd.read_csv(f\"datasets/processed/{dataset}_processed_valid.csv\")\n",
    "tab_test = pd.read_csv(f\"datasets/processed/{dataset}_processed_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"dataset_features.json\", 'r') as file:\n",
    "    dataset_info = json.load(file)[dataset]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = dataset_info[\"categorical\"]\n",
    "real_value_columns = dataset_info[\"numerical\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in categorical_columns:\n",
    "    tab_all[k] = tab_all[k].astype(\"object\")\n",
    "    tab_train[k] = tab_train[k].astype(\"object\")\n",
    "    tab_valid[k] = tab_valid[k].astype(\"object\")\n",
    "    tab_test[k] = tab_test[k].astype(\"object\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.preprocessing\n",
    "\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_case_ids(tab):\n",
    "    return list(tab[\"CaseID\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import tensor, max, int64, float32\n",
    "from torch_geometric.data import HeteroData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot_encoder(dataset: pd.DataFrame, key: str):\n",
    "    datas = dataset[key].unique()\n",
    "    datas = datas.reshape([len(datas), 1])\n",
    "    onehot = sklearn.preprocessing.OneHotEncoder()\n",
    "    onehot.fit(datas)\n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot_encodings(\n",
    "    onehot, datas: pd.Series\n",
    "):\n",
    "    return onehot.transform(datas.reshape(-1, 1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_features(dataset: pd.DataFrame, trace: pd.DataFrame, cat_features, real_features) -> dict:\n",
    " \n",
    "\n",
    "    res = {}\n",
    "\n",
    "    for key in trace:\n",
    "        values = trace[key].values\n",
    "        if key in cat_features:\n",
    "            onehot_encoder = get_one_hot_encoder(dataset, key)\n",
    "            try:\n",
    "                res[key] = tensor(\n",
    "                    get_one_hot_encodings(onehot_encoder, values),\n",
    "                    dtype=float32,\n",
    "                    requires_grad=True\n",
    "                )\n",
    "            except ValueError:\n",
    "                print(key)\n",
    "                print(values)\n",
    "        if key in real_features:\n",
    "            res[key] = tensor(values,  dtype=float32,requires_grad=True)\n",
    "            res[key] = res[key].reshape(res[key].shape[0], 1)\n",
    "        \n",
    "    \n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compute_edges_indexs(node_features: dict, prefix_len):\n",
    "    res = {}\n",
    "    keys = node_features.keys()\n",
    "    \n",
    "    indexes = [[i, i + 1] for i in range(prefix_len-1)]\n",
    "   \n",
    "    for k in keys:\n",
    "        if len(node_features[k]) != 1:\n",
    "            if k == \"Activity\":\n",
    "                res[(k, \"followed_by\", k)] = indexes\n",
    "                for k2 in keys:\n",
    "                    if k2 != k:\n",
    "                        if len(node_features[k2]) == 1:\n",
    "                            res[(k, \"related_to\", k2)] = [\n",
    "                                [i, 0] for i in range(prefix_len)\n",
    "                            ]\n",
    "                        else:\n",
    "                            res[(k, \"related_to\", k2)] = [\n",
    "                                [i, i] for i in range(prefix_len)\n",
    "                            ]\n",
    "            else:\n",
    "                res[(k, \"related_to\", k)] = indexes\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def build_prefixes_graph_from_trace(dataset, trace, cat_features, real_features, prefix_length):\n",
    "    X = []  # graphs\n",
    "   \n",
    "    \n",
    "    \n",
    "    node_features = get_node_features(dataset, trace, cat_features, real_features)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    G = HeteroData()\n",
    "        \n",
    "        \n",
    "        \n",
    "    for k in node_features:\n",
    "        if k != \"case:label\":\n",
    "            G[k].x = node_features[k][:prefix_length]\n",
    "\n",
    "\n",
    "    edges_indexes = compute_edges_indexs(node_features, prefix_length)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    for k in edges_indexes:\n",
    "        ce = [[], []]\n",
    "        for i in range(len(edges_indexes[k])):\n",
    "            ce[0].append(edges_indexes[k][i][0])\n",
    "            ce[1].append(edges_indexes[k][i][1])\n",
    "        edges_indexes[k] = ce\n",
    "\n",
    "    for k in edges_indexes:\n",
    "        G[k].edge_index = tensor(edges_indexes[k], dtype=int64)\n",
    "\n",
    "\n",
    "    ## Get the label of the trace\n",
    "\n",
    "    G.y = {}\n",
    "        \n",
    "    ##\n",
    "        \n",
    "        \n",
    "        \n",
    "    X.append(G)\n",
    "    \n",
    "    \n",
    "\n",
    "        \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_train_ids = get_case_ids(tab_train)\n",
    "case_valid_ids = get_case_ids(tab_valid)\n",
    "case_test_ids = get_case_ids(tab_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(case_train_ids))\n",
    "print(len(case_valid_ids))\n",
    "print(len(case_test_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_train[\"CaseID\"] = tab_train[\"CaseID\"].astype(np.str_)\n",
    "tab_valid[\"CaseID\"] = tab_valid[\"CaseID\"].astype(np.str_)\n",
    "tab_test[\"CaseID\"] = tab_test[\"CaseID\"].astype(np.str_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = (\n",
    "        tab_train.query(f\"CaseID == '{case_train_ids[0]}'\")\n",
    "        .reset_index()\n",
    "        .drop(columns=\"index\")\n",
    "        .drop(columns=\"CaseID\")\n",
    "    )\n",
    "trace "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX_LENGTH = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preparing training dataset...\")\n",
    "\n",
    "X_train = []\n",
    "\n",
    "\n",
    "for i in tqdm(range(len(case_train_ids))):\n",
    "    trace = (\n",
    "        tab_train.query(f\"CaseID == '{case_train_ids[i]}'\")\n",
    "        .reset_index()\n",
    "        .drop(columns=\"index\")\n",
    "        .drop(columns=\"CaseID\")\n",
    "    )\n",
    "\n",
    "    if len(trace) > 2:\n",
    "        graphs = build_prefixes_graph_from_trace(\n",
    "            dataset=tab_all,\n",
    "            trace=trace,\n",
    "            cat_features=categorical_columns,\n",
    "            real_features=real_value_columns,\n",
    "            prefix_length=PREFIX_LENGTH,\n",
    "        )\n",
    "        for i in range(len(graphs)):\n",
    "            X_train.append(graphs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dir_graphs + dataset + \"_TRAIN_repair.pkl\", \"wb\") as f:\n",
    "    pickle.dump(X_train, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preparing validation dataset...\")\n",
    "\n",
    "X_valid = []\n",
    "\n",
    "\n",
    "for i in tqdm(range(len(case_valid_ids))):\n",
    "    trace = (\n",
    "        tab_valid.query(f\"CaseID == '{case_valid_ids[i]}'\")\n",
    "        .reset_index()\n",
    "        .drop(columns=\"index\")\n",
    "        .drop(columns=\"CaseID\")\n",
    "    )\n",
    "    if len(trace) > 2:\n",
    "        graphs = build_prefixes_graph_from_trace(\n",
    "            dataset=tab_all,\n",
    "            trace=trace,\n",
    "            cat_features=categorical_columns,\n",
    "            real_features=real_value_columns,\n",
    "            prefix_length=PREFIX_LENGTH\n",
    "        )\n",
    "        for i in range(len(graphs)):\n",
    "            X_valid.append(graphs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dir_graphs + dataset + \"_VALID_repair.pkl\", \"wb\") as f:\n",
    "    pickle.dump(X_valid, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preparing test dataset...\")\n",
    "\n",
    "X_test = []\n",
    "\n",
    "\n",
    "for i in tqdm(range(len(case_test_ids))):\n",
    "    trace = (\n",
    "        tab_test.query(f\"CaseID == '{case_test_ids[i]}'\")\n",
    "        .reset_index()\n",
    "        .drop(columns=\"index\")\n",
    "        .drop(columns=\"CaseID\")\n",
    "    )\n",
    "\n",
    "    if len(trace) > 2:\n",
    "        graphs = build_prefixes_graph_from_trace(\n",
    "            dataset=tab_all,\n",
    "            trace=trace,\n",
    "            cat_features=categorical_columns,\n",
    "            real_features=real_value_columns,\n",
    "            prefix_length=PREFIX_LENGTH\n",
    "        )\n",
    "        for i in range(len(graphs)):\n",
    "            X_test.append(graphs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dir_graphs + dataset + \"_TEST_repair.pkl\", \"wb\") as f:\n",
    "    pickle.dump(X_test, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hgnn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
